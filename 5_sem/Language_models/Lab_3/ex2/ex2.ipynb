{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Subtask 1 - PAPUGA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BAD', 'Hotel nie powinien mieć ich 5.'), ('GOOD', 'Polecam i mam nadzieję, że taki poziom już pozostanie.')]\n",
      "[('BAD', 'Mało cierpliwy i nie ma moim zdaniem dobrego podejścia do pacjentek.'), ('BAD', 'Bede musiał skorzystać z porady innego lekarza, ta wizyta nic nie wniosła, zwraca czasu i pieniędzy.')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_reviews(file_name):\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.rstrip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "def shuffle_data(data):\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "\n",
    "    return [(s.split(\" \")[0], \" \".join(s.split(\" \")[1:])) for s in data]\n",
    "\n",
    "\n",
    "\n",
    "data = get_reviews(\"reviews.txt\")\n",
    "shuffled_data = shuffle_data(data)\n",
    "\n",
    "lines = split_data(shuffled_data)\n",
    "\n",
    "N = len(lines)\n",
    "test_size = N // 4\n",
    "train_size = N - test_size\n",
    "\n",
    "train_lines = lines[:train_size]\n",
    "test_lines = lines[test_size:]\n",
    "\n",
    "print(test_lines[:2])\n",
    "print(train_lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.12572303, 0), (0.5, 0), (0.5, 1), (0.75667584, 1), (0.28219464, 0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_probability import determitive_prob\n",
    "\n",
    "\n",
    "def return_predictions(data):\n",
    "    res = []\n",
    "    for mark, rev in data:\n",
    "        prob = determitive_prob(rev)\n",
    "        y = 1 if mark == \"GOOD\" else 0\n",
    "        res.append((prob, y))\n",
    "    return res\n",
    "\n",
    "\n",
    "predictions_train = return_predictions(train_lines)\n",
    "\n",
    "\n",
    "predictions_test = return_predictions(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for y_pred, y in predictions:\n",
    "    y_pred_res = 0\n",
    "    if y_pred >= 0.5:\n",
    "        y_pred_res = 1\n",
    "    else:\n",
    "        y_pred_res = 0\n",
    "    if y_pred_res == y:\n",
    "        correct += 1\n",
    "\n",
    "correct / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 0.62\n",
      "Accuracy Test 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predictions_test[0]\n",
    "y_true = [y for x, y in predictions_test]\n",
    "y_pred = [1 if x >= 0.5 else 0 for x, y in predictions_test]\n",
    "\n",
    "y_true_test = [y for x, y in predictions_train]\n",
    "y_pred_test = [1 if x >= 0.5 else 0 for x, y in predictions_train]\n",
    "\n",
    "accuracy_p = accuracy_score(y_true, y_pred)\n",
    "accuracy_p_test = accuracy_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy Train: {accuracy_p:.2}\")\n",
    "print(f\"Accuracy Test {accuracy_p_test:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Subtask 2 - BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer_B = AutoTokenizer.from_pretrained(model_name)\n",
    "model_B = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(L):\n",
    "    txt = \" \".join(L)\n",
    "    input_ids = tokenizer_B(txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    output = model_B(input_ids=input_ids)\n",
    "    return output.last_hidden_state.detach().cpu().numpy()[0, 0, :]\n",
    "\n",
    "\n",
    "\n",
    "def get_reviews(file_name):\n",
    "\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.rstrip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "lines = get_reviews(\"reviews.txt\")\n",
    "\n",
    "\n",
    "\n",
    "def BERT_predict(t_lines):\n",
    "    X_t, y_t = [], []\n",
    "    for mark, rev in t_lines:\n",
    "        y = 0 if mark == \"BAD\" else 1\n",
    "        x = representation(rev)\n",
    "        X_t.append(x)\n",
    "        y_t.append(y)\n",
    "    return X_t, y_t\n",
    "\n",
    "\n",
    "X_train_BERT, y_train_BERT = BERT_predict(train_lines)\n",
    "X_test_BERT, y_test_BERT = BERT_predict(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9266666666666666\n",
      "Test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\").fit(X_train_BERT, y_train_BERT)\n",
    "\n",
    "print(\"Train accuracy:\", clf.score(X_train_BERT, y_train_BERT))\n",
    "print(\"Test accuracy:\", clf.score(X_test_BERT, y_test_BERT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Subtask 3 - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPUGA_PROBS_train = return_predictions(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array([emb.flatten() for emb in X_train_BERT])\n",
    "y_train = np.array([1 if prob >= 0.5 else 0 for prob, y in PAPUGA_PROBS_train])\n",
    "\n",
    "\n",
    "X_test, y_test = [], []\n",
    "for mark, rev in test_lines:\n",
    "    y = 1 if mark == \"GOOD\" else 0\n",
    "\n",
    "    X_test.append(representation(rev))\n",
    "    y_test.append(y)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# print(X_test[0], y_test[0])\n",
    "# print(X_train[0], y_train[0])\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Precssion: 0.53\n",
      "Recall: 0.99\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precission = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2}\")\n",
    "print(f\"Precssion: {precission:.2}\")\n",
    "print(f\"Recall: {recall:.2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
