{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"flax-community/papuGaPT2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Bezkontekstowe osadzenia słów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings_papuga(word: str):\n",
    "\n",
    "    word = \" \" + word\n",
    "    tokens_ids = tokenizer(word, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    n = len(tokens_ids)\n",
    "    embeddings = model.transformer.wte.weight.detach().cpu().numpy()\n",
    "\n",
    "    weights = np.exp(-np.arange(n, dtype=float) * 0.5)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    word_emb = np.zeros(768)\n",
    "    for emd_token, weight in zip(embeddings[tokens_ids], weights):\n",
    "        scaled_token = emd_token * weight\n",
    "        word_emb += scaled_token\n",
    "\n",
    "    return word_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERED_TEXT = \"clustered_text.txt\"\n",
    "\n",
    "\n",
    "with open(CLUSTERED_TEXT, \"r\", encoding=\"utf-8\") as file:\n",
    "    clusters_txt = file.read()\n",
    "\n",
    "WORDS = {}\n",
    "\n",
    "for x in clusters_txt.split(\"\\n\"):\n",
    "    L = x.split()\n",
    "    if len(L) < 2:\n",
    "        continue\n",
    "    WORDS[L[0]] = L[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"output\\word_embedings_file.txt\"\n",
    "\n",
    "\n",
    "def paste_embedding_into_file(file_name):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        pass\n",
    "    words = [w for word in WORDS.values() for w in word]\n",
    "\n",
    "    for w in words:\n",
    "        w_emb = get_word_embeddings_papuga(w)\n",
    "\n",
    "        word_emb_str = \"\"\n",
    "        for n in w_emb:\n",
    "            word_emb_str = f\"{word_emb_str} {n}\"\n",
    "        with open(file_name, \"a\") as file:\n",
    "            print(f\"{w}{word_emb_str}\", file=file)\n",
    "\n",
    "\n",
    "paste_embedding_into_file(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.595764\n"
     ]
    }
   ],
   "source": [
    "from word_emb_evaluation import benchmark\n",
    "\n",
    "benchmark(CLUSTERED_TEXT=CLUSTERED_TEXT, CALCULATED_EMBEDINGS=OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Kontekstowe osadzenia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token: 'owo'\n",
      "Embedding: tensor([-0.0063,  0.1560, -0.1094,  0.4298,  0.0949], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Token: 'c'\n",
      "Embedding: tensor([-0.8746,  0.3371, -0.9575,  0.4812,  1.0283], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Token: 'pomara'\n",
      "Embedding: tensor([-0.1905,  0.2520, -0.2108,  0.0751,  0.2352], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Token: 'ńcza'\n",
      "Embedding: tensor([-0.6236,  0.0452, -0.1211,  0.1778,  0.6638], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "text = \"owoc pomarańcza\"\n",
    "token_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "tokens = [tokenizer.decode(idx) for idx in token_ids][1:-1]\n",
    "outputs = model(token_ids.unsqueeze(0).to(device))\n",
    "embeddings = outputs.last_hidden_state[0][1:-1]\n",
    "\n",
    "\n",
    "for token, embedding in zip(tokens, embeddings):\n",
    "\n",
    "    print(f\"\\nToken: '{token}'\")\n",
    "\n",
    "    print(f\"Embedding: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, model, tokenizer):\n",
    "    tokens_ids = tokenizer(word, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_input_embeddings()(tokens_ids)\n",
    "    word_embedding = embeddings.mean(dim=0).squeeze().numpy()\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "## Wyniki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'piśmiennicze:': ['pisak', 'flamaster', 'ołówek', 'długopis', 'pióro'], 'małe_ssaki:': ['mysz', 'szczur', 'chomik', 'łasica', 'kuna', 'bóbr'], 'okręty:': ['niszczyciel', 'lotniskowiec', 'trałowiec', 'krążownik', 'pancernik', 'fregata', 'korweta'], 'lekarze:': ['lekarz', 'pediatra', 'ginekolog', 'kardiolog', 'internista', 'geriatra'], 'zupy:': ['rosół', 'żurek', 'barszcz'], 'uczucia:': ['miłość', 'przyjaźń', 'nienawiść', 'gniew', 'smutek', 'radość', 'strach'], 'działy_matematyki:': ['algebra', 'analiza', 'topologia', 'logika', 'geometria'], 'budynki_sakralne:': ['kościół', 'bazylika', 'kaplica', 'katedra', 'świątynia', 'synagoga', 'zbór'], 'stopień_wojskowy:': ['chorąży', 'podporucznik', 'porucznik', 'kapitan', 'major', 'pułkownik', 'generał', 'podpułkownik'], 'grzyby_jadalne:': ['pieczarka', 'borowik', 'gąska', 'kurka', 'boczniak', 'kania'], 'prądy_filozoficzne:': ['empiryzm', 'stoicyzm', 'racjonalizm', 'egzystencjalizm', 'marksizm', 'romantyzm'], 'religie:': ['chrześcijaństwo', 'buddyzm', 'islam', 'prawosławie', 'protestantyzm', 'kalwinizm', 'luteranizm', 'judaizm'], 'dzieła_muzyczne:': ['sonata', 'synfonia', 'koncert', 'preludium', 'fuga', 'suita'], 'cyfry:': ['jedynka', 'dwójka', 'trójka', 'czwórka', 'piątka', 'szóstka', 'siódemka', 'ósemka', 'dziewiątka'], 'owady:': ['ważka', 'biedronka', 'żuk', 'mrówka', 'mucha', 'osa', 'pszczoła', 'chrząszcz'], 'broń_biała:': ['miecz', 'topór', 'sztylet', 'nóż', 'siekiera'], 'broń_palna:': ['karabin', 'pistolet', 'rewolwer', 'fuzja', 'strzelba'], 'komputery:': ['komputer', 'laptop', 'kalkulator', 'notebook'], 'kolory:': ['biel', 'żółć', 'czerwień', 'błękit', 'zieleń', 'brąz', 'czerń'], 'duchowny:': ['wikary', 'biskup', 'ksiądz', 'proboszcz', 'rabin', 'pop', 'arcybiskup', 'kardynał', 'pastor'], 'ryby:': ['karp', 'śledź', 'łosoś', 'dorsz', 'okoń', 'sandacz', 'szczupak', 'płotka'], 'napoje_mleczne:': ['jogurt', 'kefir', 'maślanka'], 'czynności_sportowe:': ['bieganie', 'skakanie', 'pływanie', 'maszerowanie', 'marsz', 'trucht'], 'ubranie:': ['garnitur', 'smoking', 'frak', 'żakiet', 'marynarka', 'koszula', 'bluzka', 'sweter', 'sweterek', 'sukienka', 'kamizelka', 'spódnica', 'spodnie'], 'mebel:': ['krzesło', 'fotel', 'kanapa', 'łóżko', 'wersalka', 'sofa', 'stół', 'stolik', 'ława'], 'przestępca:': ['morderca', 'zabójca', 'gwałciciel', 'złodziej', 'bandyta', 'kieszonkowiec', 'łajdak', 'łobuz'], 'mięso_wędliny': ['wieprzowina', 'wołowina', 'baranina', 'cielęcina', 'boczek', 'baleron', 'kiełbasa', 'szynka', 'schab', 'karkówka', 'dziczyzna'], 'drzewo:': ['dąb', 'klon', 'wiąz', 'jesion', 'świerk', 'sosna', 'modrzew', 'platan', 'buk', 'cis', 'jawor', 'jarzębina', 'akacja'], 'źródło_światła:': ['lampa', 'latarka', 'lampka', 'żyrandol', 'żarówka', 'reflektor', 'latarnia', 'lampka'], 'organ:': ['wątroba', 'płuco', 'serce', 'trzustka', 'żołądek', 'nerka', 'macica', 'jajowód', 'nasieniowód', 'prostata', 'śledziona'], 'oddziały:': ['kompania', 'pluton', 'batalion', 'brygada', 'armia', 'dywizja', 'pułk'], 'napój_alkoholowy:': ['piwo', 'wino', 'wódka', 'dżin', 'nalewka', 'bimber', 'wiśniówka', 'cydr', 'koniak', 'wiśniówka'], 'kot_drapieżny:': ['puma', 'pantera', 'lampart', 'tygrys', 'lew', 'ryś', 'żbik', 'gepard', 'jaguar'], 'metal:': ['żelazo', 'złoto', 'srebro', 'miedź', 'nikiel', 'cyna', 'cynk', 'potas', 'platyna', 'chrom', 'glin', 'aluminium'], 'samolot:': ['samolot', 'odrzutowiec', 'awionetka', 'bombowiec', 'myśliwiec', 'samolocik', 'helikopter', 'śmigłowiec'], 'owoc:': ['jabłko', 'gruszka', 'śliwka', 'brzoskwinia', 'cytryna', 'pomarańcza', 'grejpfrut', 'porzeczka', 'nektaryna'], 'pościel:': ['poduszka', 'prześcieradło', 'kołdra', 'kołderka', 'poduszeczka', 'pierzyna', 'koc', 'kocyk', 'pled'], 'agd:': ['lodówka', 'kuchenka', 'pralka', 'zmywarka', 'mikser', 'sokowirówka', 'piec', 'piecyk', 'piekarnik']}\n"
     ]
    }
   ],
   "source": [
    "CLUSTERED_TEXT = \"clustered_text.txt\"\n",
    "\n",
    "\n",
    "with open(CLUSTERED_TEXT, \"r\", encoding=\"utf-8\") as file:\n",
    "    clusters_txt = file.read()\n",
    "\n",
    "WORDS = {}\n",
    "\n",
    "for x in clusters_txt.split(\"\\n\"):\n",
    "    L = x.split()\n",
    "    if len(L) < 2:\n",
    "        continue\n",
    "    WORDS[L[0]] = L[1:]\n",
    "\n",
    "print(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"output\\word_embeddings_context.txt\"\n",
    "\n",
    "# czyszczenie pliku\n",
    "with open(OUTPUT_FILE, \"w\") as file:\n",
    "    pass\n",
    "\n",
    "\n",
    "# zapisanie policzonych embedingów\n",
    "def calulate_context_embedings(file_name):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for k, wrds in WORDS.items():\n",
    "            for w in wrds:\n",
    "                w_emb = get_word_embedding(w, model, tokenizer)\n",
    "                word_emb_str = \" \".join(map(str, w_emb.tolist()))\n",
    "                print(f\"{w} {word_emb_str}\", file=file)\n",
    "\n",
    "\n",
    "calulate_context_embedings(file_name=OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start ABX tests\n",
      "TOTAL SCORE: 0.63291\n"
     ]
    }
   ],
   "source": [
    "from word_emb_evaluation import benchmark\n",
    "\n",
    "benchmark(CLUSTERED_TEXT=CLUSTERED_TEXT, CALCULATED_EMBEDINGS=OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "# Testy ABX dla osadzeń kontekstowych dla zniesztłaconych danych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "## Piewszy typ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'piśmiennicze:': ['sipak', 'flmaaster', 'olowke', 'ilugopds', 'iporo'], 'małe_ssaki:': ['symz', 'szczru', 'cmohik', 'alsica', 'knua', 'brbo'], 'okręty:': ['nyszcziciel', 'lotnickowies', 'ecalowitr', 'koawrznik', 'canpernik', 'rfegata', 'kowreta'], 'lekarze:': ['lezark', 'petiadra', 'gkneiolog', 'kargiolod', 'internsita', 'gertaria'], 'zupy:': ['orsol', 'zuerk', 'bzrsacz'], 'uczucia:': ['molisc', 'pjzynazr', 'niencwisa', 'gniwe', 'skutem', 'cadosr', 'shract'], 'działy_matematyki:': ['blgeara', 'anailza', 'lopoaogit', 'logiak', 'gtomreeia'], 'budynki_sakralne:': ['coskiol', 'bklyziaa', 'kapliac', 'ktaedra', 'awiatynis', 'anyagogs', 'bzor'], 'stopień_wojskowy:': ['ahorczy', 'podporukznic', 'pokucznir', 'kapatin', 'maojr', 'pulkownik', 'genarel', 'ppdouokwlnik'], 'grzyby_jadalne:': ['paeczirka', 'borokiw', 'gaska', 'aurkk', 'obczniak', 'aknia'], 'prądy_filozoficzne:': ['emyirpzm', 'stozcymi', 'racaonjlizm', 'egzystmzcjaline', 'amrksizm', 'rtmmnoyza'], 'religie:': ['chrzescjianstwo', 'bdduyzm', 'ialsm', 'prawoslwaie', 'yrotestantpzm', 'kwlainizm', 'luterainzm', 'jaduizm'], 'dzieła_muzyczne:': ['sanota', 'syifonna', 'kocnert', 'prluedium', 'gufa', 'auits'], 'cyfry:': ['jedyakn', 'kwojda', 'rtojka', 'ckworza', 'kiatpa', 'szosakt', 'sdoiemka', 'oesmka', 'dzaweiitka'], 'owady:': ['wakza', 'kiedronba', 'kuz', 'morwka', 'mcuha', 'soa', 'oszcpzla', 'chazrszcz'], 'broń_biała:': ['imecz', 'tpoor', 'setylzt', 'nzo', 'aiekiesr'], 'broń_palna:': ['narabik', 'poltiset', 'rewoewlr', 'ufzja', 'ztsrelba'], 'komputery:': ['mokpuetr', 'lpptoa', 'kalkuoatlr', 'ntoebook'], 'kolory:': ['lieb', 'zloc', 'cwerzien', 'klebit', 'znelei', 'barz', 'zcern'], 'duchowny:': ['wiakry', 'sibkup', 'ksiazd', 'rpobozzcs', 'rbain', 'opp', 'arcybsikpu', 'kardanyl', 'saptor'], 'ryby:': ['rakp', 'sldez', 'losos', 'dozsr', 'ookn', 'sandacz', 'szczuapk', 'paotkl'], 'napoje_mleczne:': ['jogrut', 'kerif', 'maslanak'], 'czynności_sportowe:': ['bieeagin', 'skakanei', 'pylwanie', 'mzsaenowarei', 'mzrsa', 'trucht'], 'ubranie:': ['garnrtui', 'smogink', 'fkar', 'ztkiea', 'mrrynaaka', 'klszuoa', 'kluzba', 'tweser', 'sweteker', 'sukianek', 'kakimelza', 'spodcina', 'siodnpe'], 'mebel:': ['erzkslo', 'ftoel', 'kapana', 'lozko', 'wersakla', 'osfa', 'slot', 'ltosik', 'lawa'], 'przestępca:': ['morcerda', 'zaaojcb', 'gwaccieill', 'zldezioj', 'badnyta', 'kikszoieownec', 'lajadk', 'uoblz'], 'mięso_wędliny': ['wiwprzoeina', 'wolwoina', 'baianrna', 'cielicena', 'eoczbk', 'blaeron', 'sielbaka', 'saynkz', 'sbhac', 'kakrowka', 'dzinzyzca'], 'drzewo:': ['adb', 'knol', 'iwaz', 'jiseon', 'sriewk', 'sosna', 'mozrdew', 'plntaa', 'bku', 'csi', 'jaowr', 'jaezibrna', 'akaajc'], 'źródło_światła:': ['lmapa', 'laatrka', 'lamkpa', 'dyranzlo', 'zorawka', 'reflketor', 'latarnia', 'lkmpaa'], 'organ:': ['wbtroaa', 'pculo', 'serec', 'tzrustka', 'zoledak', 'neakr', 'maciac', 'jajowod', 'nasidoinwoe', 'srpotata', 'sladzione'], 'oddziały:': ['komnapia', 'plotun', 'boatlian', 'byrgada', 'ramia', 'aywizjd', 'pluk'], 'napój_alkoholowy:': ['oiwp', 'woni', 'wdoka', 'dizn', 'aalewkn', 'bimber', 'wianiowks', 'rydc', 'konaik', 'wisnioakw'], 'kot_drapieżny:': ['pamu', 'paetnra', 'lamprat', 'tsgryy', 'wel', 'syr', 'bzik', 'geprad', 'gajuar'], 'metal:': ['zeoazl', 'lzoto', 'srebor', 'diemz', 'nlkiei', 'ycna', 'ckny', 'aotps', 'plntyaa', 'chorm', 'ilgn', 'alumiinum'], 'samolot:': ['tamolos', 'odrzutowice', 'nwioaetka', 'bombowice', 'meiliwsyc', 'samolkoic', 'heoiklpter', 'smigioelwc'], 'owoc:': ['labjko', 'graszku', 'slikwa', 'brzoskwiain', 'rytcyna', 'pomarzncaa', 'urepjfrgt', 'porzkczea', 'entkaryna'], 'pościel:': ['kodupzsa', 'pezescirradlo', 'kaldro', 'koladrke', 'podcszezuka', 'pnerzyia', 'okc', 'kokyc', 'plde'], 'agd:': ['lodoakw', 'kuacenkh', 'pralak', 'zmarawky', 'mikesr', 'rokowisowka', 'pice', 'pceiyk', 'eipkaknir']}\n"
     ]
    }
   ],
   "source": [
    "CLUSTERED_TEXT = \"distored_clusters\\distorted_swapped.txt\"\n",
    "\n",
    "with open(CLUSTERED_TEXT, \"r\", encoding=\"utf-8\") as file:\n",
    "    clusters_txt = file.read()\n",
    "\n",
    "WORDS = {}\n",
    "\n",
    "for x in clusters_txt.split(\"\\n\"):\n",
    "    L = x.split()\n",
    "    if len(L) < 2:\n",
    "        continue\n",
    "    WORDS[L[0]] = L[1:]\n",
    "\n",
    "print(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"output\\word_embeddings_context_swapped.txt\"\n",
    "\n",
    "# czyszczenie pliku\n",
    "with open(OUTPUT_FILE, \"w\") as file:\n",
    "    pass\n",
    "\n",
    "\n",
    "calulate_context_embedings(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start ABX tests\n",
      "TOTAL SCORE: 0.539432\n"
     ]
    }
   ],
   "source": [
    "from word_emb_evaluation import benchmark\n",
    "\n",
    "benchmark(CLUSTERED_TEXT=CLUSTERED_TEXT, CALCULATED_EMBEDINGS=OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "## Drugi typ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'piśmiennicze:': ['pjsak', 'flamcster', 'omowek', 'dlugoqis', 'pjoro'], 'małe_ssaki:': ['mzsz', 'szdzur', 'chomil', 'lasicb', 'kvna', 'bobs'], 'okręty:': ['njsaczyciel', 'lotnitlpwiec', 'tralpwiec', 'krczownik', 'paoceroik', 'fregaua', 'kprweta'], 'lekarze:': ['lekara', 'pedibtra', 'ginekplog', 'kasdjolog', 'intesnista', 'gfriatrb'], 'zupy:': ['rpsol', 'zurfk', 'barszca'], 'uczucia:': ['mimosc', 'prazjazn', 'nifoawisc', 'gnjew', 'smutfk', 'radosd', 'strbch'], 'działy_matematyki:': ['alhebra', 'anamiza', 'toqolpgia', 'logikb', 'geomftrib'], 'budynki_sakralne:': ['kosciom', 'baazlika', 'kaqlica', 'katedrb', 'swiatyoja', 'syoagoga', 'zcor'], 'stopień_wojskowy:': ['ciorazy', 'podporucznil', 'porucznjk', 'kapiuan', 'majos', 'pvmkownik', 'generam', 'poepumkownik'], 'grzyby_jadalne:': ['piedzarkb', 'borowil', 'gatka', 'kurla', 'bodzniak', 'kaoia'], 'prądy_filozoficzne:': ['empirzzm', 'suoidyzm', 'racjonblizm', 'ehzyttencjalizm', 'marksjzm', 'ronantyzm'], 'religie:': ['chszescikanstwp', 'buedyzm', 'ismam', 'prawoslawjf', 'protfstanuyzm', 'kalwioizm', 'luteraoiam', 'jvdaizm'], 'dzieła_muzyczne:': ['sooata', 'syngonia', 'koncest', 'prelvdium', 'fvga', 'sujta'], 'cyfry:': ['jfdynka', 'dxojka', 'trokka', 'czxorka', 'piatkb', 'szpstka', 'siodfmkb', 'otemka', 'dziewiauka'], 'owady:': ['wazkb', 'bjedronkb', 'zvk', 'mroxka', 'mudha', 'ota', 'psacaola', 'chrzatzcz'], 'broń_biała:': ['mifcz', 'topos', 'satylet', 'npz', 'sifkiera'], 'broń_palna:': ['karacin', 'pittoleu', 'rfxolwer', 'fvzja', 'straelbb'], 'komputery:': ['komqvter', 'laqtop', 'kalkumator', 'nptebook'], 'kolory:': ['biem', 'zplc', 'czerwjen', 'blekiu', 'zjelen', 'bsaz', 'caern'], 'duchowny:': ['wikasy', 'biskuq', 'ksiada', 'prpboszcz', 'rabjn', 'ppp', 'arcycjskup', 'kasdynbl', 'pastpr'], 'ryby:': ['kbrp', 'sleda', 'lpsos', 'dortz', 'okpn', 'saodacz', 'szdaupak', 'plotkb'], 'napoje_mleczne:': ['joguru', 'kefjr', 'matlanka'], 'czynności_sportowe:': ['bkeganie', 'skalanie', 'plzwaoie', 'maszfrpwanif', 'marsa', 'tsucht'], 'ubranie:': ['garnitvr', 'smoling', 'fsak', 'zaliet', 'marznarkb', 'koszuma', 'bmuzka', 'swfter', 'sweterel', 'sukiepka', 'kamizella', 'sqodnica', 'spoenie'], 'mebel:': ['krzfslo', 'fouel', 'kanbpa', 'loako', 'wersalkb', 'spfa', 'suol', 'stomik', 'lawb'], 'przestępca:': ['moreerca', 'zabojcb', 'gwalcicjel', 'zlodziek', 'baodyta', 'kieszonkpwiec', 'lajeak', 'locuz'], 'mięso_wędliny': ['wiepszowina', 'wolqwina', 'baraninb', 'cielecjoa', 'bpczek', 'balfron', 'kielbbsa', 'saynka', 'schac', 'karkowlb', 'dzjczyzoa'], 'drzewo:': ['dbb', 'kmon', 'wjaz', 'jesjon', 'sxierk', 'sosoa', 'modrzex', 'platao', 'bul', 'cit', 'jaxor', 'jarzebioa', 'akbcja'], 'źródło_światła:': ['lampb', 'latbrka', 'lanpka', 'zyraneom', 'zaroxka', 'reflfktos', 'latarnic', 'lbmpka'], 'organ:': ['watroca', 'pmuco', 'serde', 'trzustkb', 'zomadek', 'nfrka', 'macida', 'jajoxod', 'nasifniowoe', 'prosvata', 'sleeziona'], 'oddziały:': ['kompanib', 'plutpn', 'baualion', 'bsygada', 'asmia', 'dyxizja', 'pull'], 'napój_alkoholowy:': ['pixo', 'winp', 'woeka', 'dzjn', 'namewka', 'bimbes', 'wisnjpwka', 'czdr', 'kooiak', 'wisnioyka'], 'kot_drapieżny:': ['pvma', 'paotera', 'lamparu', 'tyhrys', 'lex', 'ryt', 'zbjk', 'geqard', 'jagubr'], 'metal:': ['zelazp', 'zlotp', 'srecro', 'mieda', 'nikiem', 'cyoa', 'cyok', 'potat', 'platzna', 'chron', 'gljn', 'amuminium'], 'samolot:': ['samolpt', 'odszutowiec', 'awipnetkb', 'bpmbowiec', 'mysliwjfc', 'samolocil', 'helikoquer', 'smiglpwjec'], 'owoc:': ['jabllo', 'gruszkb', 'slixka', 'braoskwinia', 'cztryna', 'pomasancza', 'grekpfrut', 'poraeczka', 'neltbryna'], 'pościel:': ['podvszla', 'przfscieraelo', 'komdra', 'kpldeska', 'podusaeczka', 'pierayoa', 'kpc', 'kpcyk', 'plee'], 'agd:': ['lodowla', 'kucienka', 'pramka', 'zmywaska', 'mjkser', 'solowiroxka', 'pifc', 'pifcyk', 'pielasnik']}\n"
     ]
    }
   ],
   "source": [
    "CLUSTERED_TEXT = \"distored_clusters\\distorted_next_letter.txt\"\n",
    "\n",
    "with open(CLUSTERED_TEXT, \"r\", encoding=\"utf-8\") as file:\n",
    "    clusters_txt = file.read()\n",
    "\n",
    "WORDS = {}\n",
    "\n",
    "for x in clusters_txt.split(\"\\n\"):\n",
    "    L = x.split()\n",
    "    if len(L) < 2:\n",
    "        continue\n",
    "    WORDS[L[0]] = L[1:]\n",
    "\n",
    "print(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"output\\word_embeddings_context_next_letter.txt\"\n",
    "calulate_context_embedings(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start ABX tests\n",
      "TOTAL SCORE: 0.529846\n"
     ]
    }
   ],
   "source": [
    "benchmark(CLUSTERED_TEXT=CLUSTERED_TEXT, CALCULATED_EMBEDINGS=OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
