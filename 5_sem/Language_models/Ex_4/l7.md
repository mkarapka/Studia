# Zadanie 1
1. Lematyzujesz polski korpus, aby uprościć jego strukturę i zmniejszyć liczbę różnych form słów.
2. Tłumaczysz lematy na angielski, aby stworzyć dane równoległe tam, gdzie ich brakuje.
3. Łączysz teksty tłumaczone z istniejącymi tekstami równoległymi w obu językach.
4. Trenujesz model, który uczy się wspólnej przestrzeni wektorowej dla obu języków.

# Zadanie 2
https://arxiv.org/pdf/1902.04094
```
1. zaczynamy jakimś proptem, np tematem generacji: 'best city in the world'
2. potem do prompta doklajamy 'S' jakiś randomowych słów
3. następnie n*S razy (n należałoby jakoś empirycznie wybrać) losujemy pozycję 'i'
    1. dla pozycji 'i' wybieramy najlepiej pasujące słowo (można użyć pipelin'u fill-mask),
    albo np podzielić jedną część zdania względem maski i znaleźć najbliższy token do obu części
    2. wstawiamy wybrane słowo, lub token do sekwencji
    3. powtarzamy losowanie
```

# Zadanie 3
### Model n-gramowy recap:
- model językowy zakładający że ppb kolejengo tokenu zależy od prawdopowobieństw n-1 tokenów poprzednich
- mamy na początku tokeny <s> </s> oznaczające początek i koniec sekwencji

- podać 2x scenariusze wykorzystania go do traningu GPT

### Jak trenowany był GPT
1. Generative pretraining

2. Supervised fine-tuning

3. Reward Modeling (stworzenie systemu nagród dla bota)

3. Reinforcement learning from human feedback (uwzględnienie wag i maksymalizacja nagrody)

### Scenariusz I
- w 4 fazie a więc przyznawania ocen przez ludzi dla odpowiedzi wygenerowanych przez bota
moglibyśmy też policzyć ocenę na podstawie ppb tokenów z odpowiedzi policzonych przy pomocy M_ng
i też uwzględnić ten score w ocenie

- ten proces oceniania można by było też jakoś poprzeć sprawdzaniem, czy label dany przez człowieka, pokrywa się
w jakiś sposób z ppb zwrócownym przez M_ng, np sprawdzić ppb label'a?

### Scenariusz II
- w 1,2 fazie moglibyśmy predykcje GPT wzmocnić o wagi na podstawie ppb z M_ng
- w fazie 1 moglibyśmy dzięki M_ng zwalczyć powtarzalność albo bezsensowność odpowiedzi (takie odpowiedzi miałyby bardzo małe ppb)

# Zadanie 4 

- mamy BERTa, ale nie mamy osadzeń pozycyjnych
- czy do czegoś się może przydać
- czy fakt użycia słownika o dużej liczbie tokenów jest sprzyjający czy obiciążający

Architektura transofrmera w trakcie obliczeń nigdzie nie zapisuje kolejności obliczanych słów,
więc to my to musimy zrobić
- używamy tych tokenów pozycyjnych, aby przesunąć embedding np pierwszego słowa ze zdania, delikatnie w stronę
innych pierwszych słów
- podobnie dla drugiego itd
- każda pozycja powinna mieć swój identifier (swoje osadznie pozycyjne, niezależne od pozycji i inputu)
- osadzenia takie nie mogą być za duże, bo wtedy pozycyjne podobnieństwo przykryje nam sematyczne podobieństwo, stąd stosujemy
sin albo cos, bo niezależnie od ilości pozycji mamy jssno określone wartości minimalne i maksymalne (-1, 1), 
zwykły sin za często się powtarza, czyli zwiększamy jego okresowość bardzo mocno, robimy takie
operacje dla wszystkich wymiarów osadzeń pozytywnych, przeplatamy różne okresy sin i cos i zamieniamy funkcje
stosując raz sin, raz cos
- w ten sposób uzyskujemy deterministyczne osadzenia dla każdej pozycji
(opisane powyżej działa dla tekstu)
# Zadanie 7
- bierzemy model GPT
- zbiór danych - losowo wygenerowany ciąg liczb [10,100] o jakiejś ustalonej długości, oraz posortowany ciąg
- tokenizujemy pewnie po liczbach (bo liczb jest mało)
- trenujemy model GPT2 od zera
- powtórzyć kilka razy dla każdego ciągu, sprawdzić jak blisko on jest do ciągu wynikowego
- samplujemy tak długo aż dojdziemy do znaku końcowego
< nieposortowany ciąg, posortowany ciąd >
- sprawdzamy accuracy

# zadanie 8 
- znowu niewytrenowany model
- -> to XOR
- tokeny: START, END, ->, 0, 0
- 01 -> 1, trenujemy
ALTERNATYWNIE
- chodzi o XORowanie całych liczb, ciągów 0 i 1
- czy transformer potrafi się nauczyć języka
0 0 1 1 1 0 0 =xor po wszystkich bitach = 1
0 1 1 1 1 0 0 =xor = 0
- to tak średnio, ale możeby go nauczyć jakiegoś algorytmu, który poxoruje rzeczy i to może będzie umiał